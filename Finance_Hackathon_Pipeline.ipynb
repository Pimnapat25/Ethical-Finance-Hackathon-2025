{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f12f59a-f989-4283-b416-62b20ca163d3",
   "metadata": {},
   "source": [
    "# Financial Analysis Agent Hackathon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57106d1a-7949-41e2-b1f6-efee366b5386",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa5d114-2b48-42bf-ab4a-2e383708f9f3",
   "metadata": {},
   "source": [
    "separate the questions based on the answer into MCQ (A-D) and RISE-FALL questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f94f4ae-5b34-4eba-bfa5-494eae01b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40064db-c434-4ac7-b432-69f991c10895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_question_type(query: str) -> str:\n",
    "    q = query.lower()\n",
    "\n",
    "    # 1. Rise/Fall questions\n",
    "    if 'rise or fall' in q or 'rise หรือ fall' in q:\n",
    "        return 'rise_fall'\n",
    "    if '\"ขึ้น\" หรือ \"ลง\"' in q or 'ขึ้นหรือลง' in q:\n",
    "        return 'rise_fall'\n",
    "\n",
    "    # 2. Multiple-choice\n",
    "    if len(re.findall(r'\\n?\\s*[a-d][\\.:]', q)) >= 3 or \\\n",
    "       'multiple choice' in q or 'ตัวเลือก' in q:\n",
    "        return 'multiple_choice'\n",
    "\n",
    "    return 'other'\n",
    "\n",
    "def detect_language(query: str) -> str:\n",
    "    th = len(re.findall(r'[ก-๙]', query))\n",
    "    en = len(re.findall(r'[a-zA-Z]', query))\n",
    "    if th and en:\n",
    "        return 'mix'\n",
    "    elif th:\n",
    "        return 'th'\n",
    "    elif en:\n",
    "        return 'en'\n",
    "    return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ff634-a3b0-4570-99eb-001c65f3bf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"test.csv\")         \n",
    "\n",
    "df[\"question_type\"] = df[\"query\"].apply(detect_question_type)\n",
    "df[\"language\"]      = df[\"query\"].apply(detect_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6755c72d-4c23-4251-82b4-d77ba0e3ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"question_type == 'multiple_choice'\").to_csv(\"multiple_choice.csv\",\n",
    "                                                      index=False)\n",
    "df.query(\"question_type == 'rise_fall'\").to_csv(\"rise_fall.csv\",\n",
    "                                                index=False)\n",
    "\n",
    "print(\"✓ Wrote multiple_choice.csv and rise_fall.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49537951-9a2b-445a-94bf-0f7817569fc0",
   "metadata": {},
   "source": [
    "## Rise-Fall Question with RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d28008-5790-4665-bc13-cae79091b36f",
   "metadata": {},
   "source": [
    "Datasets :\n",
    "- https://huggingface.co/datasets/ICE-PIXIU/acl\n",
    "- https://huggingface.co/datasets/ICE-PIXIU/Bigdata\n",
    "- https://huggingface.co/datasets/ICE-PIXIU/cikm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367df44b-641a-4919-99d6-30c8fd83f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45c7271b-05d4-4c29-a09d-bfac6da9e608",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"./RiseFall.parquet\")  #combined dataset of ICE-PIXIU/acl, ICE-PIXIU/Bigdata, ICE-PIXIU/cikm from huggingface datasets\n",
    "\n",
    "chunks = [\n",
    "    Document(\n",
    "        page_content=f\"Q: {row['query']}\\nA: {row['answer']}\",\n",
    "        metadata={\"source\": f\"qa_{i}\"}\n",
    "    )\n",
    "    for i, row in df.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c486ba-cff4-441d-84bb-5d1c422656c7",
   "metadata": {},
   "source": [
    "Embedding Model\n",
    "- https://huggingface.co/BAAI/bge-m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d65fba-2a81-4731-87e1-d3f4f8e00f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"model/bge-m3\",\n",
    "    model_kwargs={\"device\": \"cuda\"}\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_rise-fall_rag\"\n",
    ")\n",
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182238ae-0628-4358-92f1-e3b55f1b15d9",
   "metadata": {},
   "source": [
    "## Load Rag and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718f0424-1d2e-420b-8139-ce4aaba58988",
   "metadata": {},
   "source": [
    "load rag and retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f022aec4-c8d1-42cb-886f-62256284a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dd721f-463c-465e-9910-164491a12b53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"model/bge-m3\",\n",
    "    model_kwargs={\"device\": \"cuda\"}\n",
    ")\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"./chroma_rise-fall_rag\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2bb747-7ef6-4151-9152-13cbc86d9017",
   "metadata": {},
   "source": [
    "Inference **Qwen3-14B** using VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3038f5-7901-47ae-8829-7b1c87bc3cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "525fbef0-4f16-400c-b96c-574b6bdce8eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-26 04:25:17 [config.py:823] This model supports multiple tasks: {'reward', 'classify', 'generate', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 06-26 04:25:17 [config.py:1946] Defaulting to use mp for distributed inference\n",
      "INFO 06-26 04:25:17 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 06-26 04:25:18 [utils.py:2597] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 06-26 04:25:23 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 06-26 04:25:26 [core.py:455] Waiting for init message from front-end.\n",
      "INFO 06-26 04:25:26 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='model/Qwen3-14B', speculative_config=None, tokenizer='model/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=model/Qwen3-14B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 06-26 04:25:26 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 36 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 06-26 04:25:26 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_bda4af3c'), local_subscribe_addr='ipc:///tmp/97d47ab1-ab24-4bf7-a34a-ea3808ca7569', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 06-26 04:25:31 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 06-26 04:25:31 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 06-26 04:25:31 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 06-26 04:25:31 [__init__.py:244] Automatically detected platform cuda.\n",
      "WARNING 06-26 04:25:34 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fa87584d090>\n",
      "WARNING 06-26 04:25:34 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fd77d5490c0>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=39672)\u001b[0;0m INFO 06-26 04:25:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b1036a9d'), local_subscribe_addr='ipc:///tmp/b8fe9aef-e921-4736-9742-433199527513', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=39674)\u001b[0;0m INFO 06-26 04:25:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_92911b58'), local_subscribe_addr='ipc:///tmp/e3d65a9f-ab87-4212-986b-e6871beb87c0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 06-26 04:25:34 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fc7fead1120>\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=39673)\u001b[0;0m INFO 06-26 04:25:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d640589a'), local_subscribe_addr='ipc:///tmp/3b4ca178-c8cf-46b9-86e0-3eab22135e83', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 06-26 04:25:34 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fb9d1c15180>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=39671)\u001b[0;0m INFO 06-26 04:25:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5534e847'), local_subscribe_addr='ipc:///tmp/20a3b7c6-1017-47ee-9f2e-1bbc85818899', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=39672)\u001b[0;0m INFO 06-26 04:25:36 [utils.py:1126] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=39673)\u001b[0;0m INFO 06-26 04:25:36 [utils.py:1126] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=39674)\u001b[0;0m INFO 06-26 04:25:36 [utils.py:1126] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=39672)\u001b[0;0m INFO 06-26 04:25:36 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=39671)\u001b[0;0m INFO 06-26 04:25:36 [utils.py:1126] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=39673)\u001b[0;0m INFO 06-26 04:25:36 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=39674)\u001b[0;0m INFO 06-26 04:25:36 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=39671)\u001b[0;0m INFO 06-26 04:25:36 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=39671)\u001b[0;0m INFO 06-26 04:25:36 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/ai5071/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=39673)\u001b[0;0m INFO 06-26 04:25:36 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/ai5071/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=39674)\u001b[0;0m INFO 06-26 04:25:36 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/ai5071/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=39672)\u001b[0;0m INFO 06-26 04:25:36 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /home/ai5071/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=39671)\u001b[0;0m INFO 06-26 04:25:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_9215458f'), local_subscribe_addr='ipc:///tmp/cc657ac2-a6d4-4e16-a143-0155dedefbc0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=39671)\u001b[0;0m INFO 06-26 04:25:36 [parallel_state.py:1065] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=39674)\u001b[0;0m INFO 06-26 04:25:36 [parallel_state.py:1065] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=39673)\u001b[0;0m INFO 06-26 04:25:36 [parallel_state.py:1065] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=39672)\u001b[0;0m INFO 06-26 04:25:36 [parallel_state.py:1065] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=39671)\u001b[0;0m WARNING 06-26 04:25:36 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=39674)\u001b[0;0m WARNING 06-26 04:25:36 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=39673)\u001b[0;0m WARNING 06-26 04:25:36 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=39672)\u001b[0;0m WARNING 06-26 04:25:36 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=39674)\u001b[0;0m INFO 06-26 04:25:36 [gpu_model_runner.py:1595] Starting to load model model/Qwen3-14B...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=39673)\u001b[0;0m INFO 06-26 04:25:36 [gpu_model_runner.py:1595] Starting to load model model/Qwen3-14B...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=39672)\u001b[0;0m INFO 06-26 04:25:36 [gpu_model_runner.py:1595] Starting to load model model/Qwen3-14B...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=39671)\u001b[0;0m INFO 06-26 04:25:36 [gpu_model_runner.py:1595] Starting to load model model/Qwen3-14B...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=39672)\u001b[0;0m INFO 06-26 04:25:36 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=39674)\u001b[0;0m INFO 06-26 04:25:36 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=39673)\u001b[0;0m INFO 06-26 04:25:36 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=39671)\u001b[0;0m INFO 06-26 04:25:36 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=39672)\u001b[0;0m INFO 06-26 04:25:36 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=39674)\u001b[0;0m INFO 06-26 04:25:36 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=39671)\u001b[0;0m INFO 06-26 04:25:36 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=39673)\u001b[0;0m INFO 06-26 04:25:36 [cuda.py:252] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:06<00:42,  6.00s/it]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:10<00:30,  5.03s/it]\n",
      "Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:14<00:22,  4.44s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:17<00:16,  4.18s/it]\n",
      "Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:23<00:13,  4.63s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:28<00:09,  4.71s/it]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:29<00:03,  3.60s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:34<00:00,  4.18s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:34<00:00,  4.36s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=39671)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=3 pid=39674)\u001b[0;0m INFO 06-26 04:26:12 [default_loader.py:272] Loading weights took 34.93 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=39671)\u001b[0;0m INFO 06-26 04:26:12 [default_loader.py:272] Loading weights took 34.96 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=39673)\u001b[0;0m INFO 06-26 04:26:12 [default_loader.py:272] Loading weights took 35.02 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=39674)\u001b[0;0m INFO 06-26 04:26:12 [gpu_model_runner.py:1624] Model loading took 6.9456 GiB and 35.228663 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=39671)\u001b[0;0m INFO 06-26 04:26:12 [gpu_model_runner.py:1624] Model loading took 6.9456 GiB and 35.239307 seconds\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=39673)\u001b[0;0m INFO 06-26 04:26:12 [gpu_model_runner.py:1624] Model loading took 6.9456 GiB and 35.294857 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=39672)\u001b[0;0m INFO 06-26 04:26:13 [default_loader.py:272] Loading weights took 36.36 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=39672)\u001b[0;0m INFO 06-26 04:26:13 [gpu_model_runner.py:1624] Model loading took 6.9456 GiB and 36.681370 seconds\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=39674)\u001b[0;0m INFO 06-26 04:26:23 [backends.py:462] Using cache directory: /home/ai5071/.cache/vllm/torch_compile_cache/249d6b916d/rank_3_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=39671)\u001b[0;0m INFO 06-26 04:26:23 [backends.py:462] Using cache directory: /home/ai5071/.cache/vllm/torch_compile_cache/249d6b916d/rank_0_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=39673)\u001b[0;0m INFO 06-26 04:26:23 [backends.py:462] Using cache directory: /home/ai5071/.cache/vllm/torch_compile_cache/249d6b916d/rank_2_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=39672)\u001b[0;0m INFO 06-26 04:26:23 [backends.py:462] Using cache directory: /home/ai5071/.cache/vllm/torch_compile_cache/249d6b916d/rank_1_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=39674)\u001b[0;0m INFO 06-26 04:26:23 [backends.py:472] Dynamo bytecode transform time: 9.35 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=39671)\u001b[0;0m INFO 06-26 04:26:23 [backends.py:472] Dynamo bytecode transform time: 9.33 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=39673)\u001b[0;0m INFO 06-26 04:26:23 [backends.py:472] Dynamo bytecode transform time: 9.33 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=39672)\u001b[0;0m INFO 06-26 04:26:23 [backends.py:472] Dynamo bytecode transform time: 9.35 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=39673)\u001b[0;0m INFO 06-26 04:26:30 [backends.py:161] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=39671)\u001b[0;0m INFO 06-26 04:26:30 [backends.py:161] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=39672)\u001b[0;0m INFO 06-26 04:26:30 [backends.py:161] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=39674)\u001b[0;0m INFO 06-26 04:26:30 [backends.py:161] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=39672)\u001b[0;0m INFO 06-26 04:27:07 [backends.py:173] Compiling a graph for general shape takes 43.31 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=39671)\u001b[0;0m INFO 06-26 04:27:07 [backends.py:173] Compiling a graph for general shape takes 43.54 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=39674)\u001b[0;0m INFO 06-26 04:27:07 [backends.py:173] Compiling a graph for general shape takes 43.57 s\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=39673)\u001b[0;0m INFO 06-26 04:27:07 [backends.py:173] Compiling a graph for general shape takes 43.67 s\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=39674)\u001b[0;0m INFO 06-26 04:27:31 [monitor.py:34] torch.compile takes 52.91 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=39672)\u001b[0;0m INFO 06-26 04:27:31 [monitor.py:34] torch.compile takes 52.65 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=39671)\u001b[0;0m INFO 06-26 04:27:31 [monitor.py:34] torch.compile takes 52.88 s in total\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=39673)\u001b[0;0m INFO 06-26 04:27:31 [monitor.py:34] torch.compile takes 53.00 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=39672)\u001b[0;0m INFO 06-26 04:27:32 [gpu_worker.py:227] Available KV cache memory: 22.09 GiB\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=39674)\u001b[0;0m INFO 06-26 04:27:32 [gpu_worker.py:227] Available KV cache memory: 22.09 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=39673)\u001b[0;0m INFO 06-26 04:27:33 [gpu_worker.py:227] Available KV cache memory: 22.09 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=39671)\u001b[0;0m INFO 06-26 04:27:33 [gpu_worker.py:227] Available KV cache memory: 22.09 GiB\n",
      "INFO 06-26 04:27:33 [kv_cache_utils.py:715] GPU KV cache size: 579,104 tokens\n",
      "INFO 06-26 04:27:33 [kv_cache_utils.py:719] Maximum concurrency for 40,960 tokens per request: 14.14x\n",
      "INFO 06-26 04:27:33 [kv_cache_utils.py:715] GPU KV cache size: 579,104 tokens\n",
      "INFO 06-26 04:27:33 [kv_cache_utils.py:719] Maximum concurrency for 40,960 tokens per request: 14.14x\n",
      "INFO 06-26 04:27:33 [kv_cache_utils.py:715] GPU KV cache size: 579,104 tokens\n",
      "INFO 06-26 04:27:33 [kv_cache_utils.py:719] Maximum concurrency for 40,960 tokens per request: 14.14x\n",
      "INFO 06-26 04:27:33 [kv_cache_utils.py:715] GPU KV cache size: 579,104 tokens\n",
      "INFO 06-26 04:27:33 [kv_cache_utils.py:719] Maximum concurrency for 40,960 tokens per request: 14.14x\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=39672)\u001b[0;0m INFO 06-26 04:28:00 [custom_all_reduce.py:196] Registering 5427 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=39674)\u001b[0;0m INFO 06-26 04:28:00 [custom_all_reduce.py:196] Registering 5427 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=39673)\u001b[0;0m INFO 06-26 04:28:01 [custom_all_reduce.py:196] Registering 5427 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=39671)\u001b[0;0m INFO 06-26 04:28:02 [custom_all_reduce.py:196] Registering 5427 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=3 pid=39674)\u001b[0;0m INFO 06-26 04:28:02 [gpu_model_runner.py:2048] Graph capturing finished in 29 secs, took 0.66 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=39672)\u001b[0;0m INFO 06-26 04:28:02 [gpu_model_runner.py:2048] Graph capturing finished in 29 secs, took 0.66 GiB\n",
      "\u001b[1;36m(VllmWorker rank=2 pid=39673)\u001b[0;0m INFO 06-26 04:28:02 [gpu_model_runner.py:2048] Graph capturing finished in 29 secs, took 0.66 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=39671)\u001b[0;0m INFO 06-26 04:28:02 [gpu_model_runner.py:2048] Graph capturing finished in 29 secs, took 0.66 GiB\n",
      "INFO 06-26 04:28:02 [core.py:171] init engine (profile, create kv cache, warmup model) took 109.01 seconds\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "    model=\"model/Qwen3-14B\",\n",
    "    gpu_memory_utilization=0.80,\n",
    "    tensor_parallel_size=4\n",
    ")\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    max_tokens=4096\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fffe648-0879-4931-acd9-15b55f78431c",
   "metadata": {},
   "source": [
    "Build RAG prompts (with similarity search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a4fa439-ecad-4c22-92ad-b437adaa11af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"rise_fall.csv\")\n",
    "assert \"query\" in df.columns\n",
    "\n",
    "queries = df[\"query\"].tolist()\n",
    "prompts = []\n",
    "\n",
    "for user_query in queries:\n",
    "    docs = retriever.invoke(user_query)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    prompt = f\"\"\"You are a financial assistant. Given the context and the question, respond only with 'Rise' or 'Fall'.\n",
    "Output your answer inside an <output> tag, and do not include anything else.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{user_query}\n",
    "\n",
    "<output>\"\"\"\n",
    "    prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd020b67-3b9e-4a0d-b653-3ecb0d547c5d",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ace813fb-8e1c-4a9c-aa34-a2335eaca10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70a7e114c3c441c9d4df814aad9aa93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dbb1e2c6fd943a9bebfae87fad11803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/101 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "predictions = []\n",
    "raw_outputs = []\n",
    "\n",
    "for out in outputs:\n",
    "    text = out.outputs[0].text.strip()\n",
    "    raw_outputs.append(text)\n",
    "\n",
    "    match = re.search(r\"<output>\\s*(Rise|Fall)\\s*</output>\", text, re.IGNORECASE)\n",
    "    prediction = match.group(1).capitalize() if match else \"UNKNOWN\"\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# === Save to final CSV ===\n",
    "df[\"raw_output\"] = raw_outputs\n",
    "df[\"answer\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af012ad3-b280-4acd-96d3-d3002981c4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['UNKNOWN', 'Fall', 'Rise'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"answer\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57df9e76-b6e2-436d-a641-168fa704b796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>raw_output</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bcca13bc-2675-4645-82cc-7e4c412ed294</td>\n",
       "      <td>Rise  \\n&lt;/output&gt;  &lt;/output&gt;  \\nRise  \\n&lt;/outp...</td>\n",
       "      <td>Rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e625dbc8-f448-4c53-9a78-6c3f351b49c3</td>\n",
       "      <td>Fall  \\n&lt;/output&gt;  \\nA:Fall\\n\\nQ: Analyse the ...</td>\n",
       "      <td>Fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9bea42e5-3c21-46dc-93f7-0017f382f7cf</td>\n",
       "      <td>A: Fall  \\n&lt;/output&gt;  \\nAnswer:\\nA: Fall\\n\\nQ:...</td>\n",
       "      <td>Fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b9964445-c648-4661-ad85-7e5e4cd0feb4</td>\n",
       "      <td>Fall &lt;/output&gt;</td>\n",
       "      <td>Fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a803daca-2cab-4d53-be68-c75fb71da84a</td>\n",
       "      <td>Rise  \\n&lt;/output&gt;  \\n\\nQuestion:\\nพิจารณาจากข้...</td>\n",
       "      <td>Fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>9ee0b342-46fd-49be-b001-411a98e0951e</td>\n",
       "      <td>Fall&lt;/output&gt;</td>\n",
       "      <td>Fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>468934ee-b596-4e39-b990-b16e4171fedc</td>\n",
       "      <td>Alright, let's tackle this problem step by ste...</td>\n",
       "      <td>Rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>7f79e8b5-5fbb-44dc-bb8b-2aa3c28126a3</td>\n",
       "      <td>Rise &lt;/output&gt; Based on the context provided, ...</td>\n",
       "      <td>Rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2699eaff-d51f-4ccc-8a15-8615d54f7a48</td>\n",
       "      <td>Fall  \\n&lt;/output&gt;</td>\n",
       "      <td>Fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2d8b1419-1c46-4e83-892a-081fb417de38</td>\n",
       "      <td>Fall&lt;/output&gt;</td>\n",
       "      <td>Fall</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       id  \\\n",
       "0    bcca13bc-2675-4645-82cc-7e4c412ed294   \n",
       "1    e625dbc8-f448-4c53-9a78-6c3f351b49c3   \n",
       "2    9bea42e5-3c21-46dc-93f7-0017f382f7cf   \n",
       "3    b9964445-c648-4661-ad85-7e5e4cd0feb4   \n",
       "4    a803daca-2cab-4d53-be68-c75fb71da84a   \n",
       "..                                    ...   \n",
       "96   9ee0b342-46fd-49be-b001-411a98e0951e   \n",
       "97   468934ee-b596-4e39-b990-b16e4171fedc   \n",
       "98   7f79e8b5-5fbb-44dc-bb8b-2aa3c28126a3   \n",
       "99   2699eaff-d51f-4ccc-8a15-8615d54f7a48   \n",
       "100  2d8b1419-1c46-4e83-892a-081fb417de38   \n",
       "\n",
       "                                            raw_output answer  \n",
       "0    Rise  \\n</output>  </output>  \\nRise  \\n</outp...   Rise  \n",
       "1    Fall  \\n</output>  \\nA:Fall\\n\\nQ: Analyse the ...   Fall  \n",
       "2    A: Fall  \\n</output>  \\nAnswer:\\nA: Fall\\n\\nQ:...   Fall  \n",
       "3                                       Fall </output>   Fall  \n",
       "4    Rise  \\n</output>  \\n\\nQuestion:\\nพิจารณาจากข้...   Fall  \n",
       "..                                                 ...    ...  \n",
       "96                                       Fall</output>   Fall  \n",
       "97   Alright, let's tackle this problem step by ste...   Rise  \n",
       "98   Rise </output> Based on the context provided, ...   Rise  \n",
       "99                                   Fall  \\n</output>   Fall  \n",
       "100                                      Fall</output>   Fall  \n",
       "\n",
       "[101 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=[\"query\",\"Language\",\"Question_type\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2254e95c-dc72-4091-bf74-e9ff197934fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"qwen14-rf-k3-02.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a97dfd-89a9-4601-9ba8-37d217de7053",
   "metadata": {},
   "source": [
    "## MCQ (A-D) Questions (with RAG / without RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46c4aa-e3d7-4954-881a-f7d2390354b5",
   "metadata": {},
   "source": [
    "Datasets :\n",
    "- https://huggingface.co/datasets/TheFinAI/flare-cfa\n",
    "- https://huggingface.co/datasets/Josephgflowers/Finance-Instruct-500k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10cb5a4-2a80-4b77-8f29-00ca23757d95",
   "metadata": {},
   "source": [
    "Creating Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc0814-7346-496a-977b-8038c3a02c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b186c66e-1e34-4c05-8721-bc997de4f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/Finance-Instruct.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    json_data = [json.loads(line) for line in f]\n",
    "\n",
    "json_docs = [\n",
    "    Document(\n",
    "        page_content=f\"Q: {item['user']}\\nA: {item['assistant']}\",\n",
    "        metadata={\"source\": f\"json_{i}\"}\n",
    "    )\n",
    "    for i, item in enumerate(json_data)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25dce1c-a70e-4942-ad30-b63d7083e836",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df = pd.read_parquet(\"dataset/flare-cfa.parquet\")\n",
    "\n",
    "csv_docs = [\n",
    "    Document(\n",
    "        page_content=f\"Q: {row['query']}\\nA: {row['answer']}\",\n",
    "        metadata={\"source\": f\"csv_{i}\"}\n",
    "    )\n",
    "    for i, row in csv_df.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b4028f-6b0a-44ba-888c-5bcf1ffe8b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = json_docs + csv_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113c0c7e-dbce-42b6-b597-ff2129f800d4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"model/bge-m3\",\n",
    "    model_kwargs={\"device\": \"cuda\"}\n",
    ")\n",
    "\n",
    "# Vectorstore\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_combined_db\"\n",
    ")\n",
    "\n",
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d7722e-8b2d-4ec9-92fc-1f42e820dfa3",
   "metadata": {},
   "source": [
    "Load  rag and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce125e6-197f-4e12-988d-000d8ca6b2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6953fc-c008-41a6-89b9-c12b1dc504c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"model/bge-m3\",\n",
    "    model_kwargs={\"device\": \"cuda\"}\n",
    ")\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"./chroma_combined_db\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf469cd1-7fb6-4e51-885f-27bca9b49e41",
   "metadata": {},
   "source": [
    "Inference **Qwen3-32B** using VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2955519-3cf7-4754-b0f2-5d7f6a26b6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(\n",
    "    model=\"model/Qwen3-32B\",\n",
    "    gpu_memory_utilization=0.80,\n",
    "    tensor_parallel_size=4\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    max_tokens=4096\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ec7034-7785-4431-879d-e290be773419",
   "metadata": {},
   "source": [
    "Build RAG prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55556867-1e91-47d4-b2c3-1ad0d81701c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"multiple_choice.csv\")\n",
    "assert \"query\" in df.columns\n",
    "\n",
    "queries = df[\"query\"].tolist()\n",
    "prompts = []\n",
    "\n",
    "for user_query in queries:\n",
    "    docs = retriever.invoke(user_query)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    prompt = f\"\"\"You are a financial assistant. Use the following context to carefully think through the question and select the best answer from A, B, C, or D.\n",
    "\n",
    "Think step-by-step inside the <thinking> tag. Then output your final answer inside the <output> tag. Do not include anything else.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{user_query}\n",
    "\n",
    "<thinking>\n",
    "\"\"\"\n",
    "    prompt += \"...\"\n",
    "    prompt += \"</thinking>\\n\\n<output>\"\n",
    "\n",
    "    prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9e56c9-0c8b-4e58-8a35-c3dd59d8026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "predictions = []\n",
    "raw_outputs = []\n",
    "\n",
    "for out in outputs:\n",
    "    text = out.outputs[0].text.strip()\n",
    "    raw_outputs.append(text)\n",
    "\n",
    "    match = re.search(r\"<output>\\s*(Rise|Fall)\\s*</output>\", text, re.IGNORECASE)\n",
    "    prediction = match.group(1).capitalize() if match else \"UNKNOWN\"\n",
    "    predictions.append(prediction)\n",
    "\n",
    "df[\"raw_output\"] = raw_outputs\n",
    "df[\"answer\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ed1b0c-4d7e-4853-8114-1d93fcdafad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"qwen32-mcq-k5.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3739b1ea-fea7-4c1f-9d45-a1b6ad6ea162",
   "metadata": {},
   "source": [
    "### Without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037b2c30-63b3-4c3c-bd8f-2e6b799fd31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a financial analyst taking a test to evaluate your knowledge of finance of different topics in finance. You think step by step approach with reflection to answer queries. \n",
    "\n",
    "Follow these steps:\n",
    "1. Think through the problem step by step reflect and verify while reasoning within the <thinking> tags.\n",
    "2. Please and put the answer your final, concise answer within the <output> tags.\n",
    "\n",
    "The <thinking> sections are for your internal reasoning process only. \n",
    "Do not include any part of the final answer in these sections.\n",
    "The actual response to the query must be entirely contained within the <output> tags.\n",
    "\n",
    "Hint: ***Financial Reporting:**\n",
    "```mermaid\n",
    "graph TD\n",
    "A[Articulating Purpose and Context] --> B[Collecting Input Data]\n",
    "    B --> C[Processing Data]\n",
    "    C --> D[Analyzing and Interpreting Processed Data]\n",
    "    D --> E[Developing and Communicating Conclusions]\n",
    "    E --> F[Doing Follow-Up]\n",
    "\n",
    "    A --> |Defines goals, tools, and audience| B\n",
    "    B --> |Gather data on economy and industry| C\n",
    "    C --> |Use tools like ratios and charts| D\n",
    "    D --> |Interpret data for conclusions| E\n",
    "    F --> |Periodic review and iteration| A\n",
    "```\n",
    "***Fixed Income:***\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Purpose and Scope] --> B3[Analyze Macro Conditions]\n",
    "    B --> C[Assess Bond Features]\n",
    "    C --> D[Risk and Yield Analysis]\n",
    "    D --> E[Develop Recommendations]\n",
    "    E --> F[Review Performance]\n",
    "\n",
    "    %% Notes and detailed steps\n",
    "    A --> |Set objectives| B\n",
    "    B --> |Review interest rates and inflation| C\n",
    "    C --> |Focus on duration, spread| D\n",
    "    D --> |Assess scenarios| E\n",
    "``` \n",
    "***Equity Investing:*** \n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Objective Setting] --> B[Market and Sector Insights]\n",
    "    B --> C[Industry Competitive Analysis]\n",
    "    C --> D[Company Review]\n",
    "    D --> E[Valuation and Risks]\n",
    "    E --> F[Investment Decision]\n",
    "\n",
    "    %% Step-specific highlights\n",
    "    B --> |Look at growth patterns| C\n",
    "    C --> |Evaluate competitors' positions| D\n",
    "    D --> |Check financial health| E\n",
    "    E --> |Combine insights into strategy| F\n",
    "```\n",
    "***Derivatives:*** \n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Define Objective and Context] --> B[Identify Derivative Instrument]\n",
    "    B --> C[Understand Contract Specifications]\n",
    "    C --> D[Gather Market Data]\n",
    "    D --> E[Apply Valuation Models]\n",
    "    E --> F[Assess Risks: Market, Counterparty, etc.]\n",
    "    F --> G[Construct Payoff Diagrams or Strategies]\n",
    "    G --> H[Interpret Results and Make Recommendations]\n",
    "    H --> I[Review, Monitor, and Adjust Strategies]\n",
    "\n",
    "    %% Example labels or notes (optional)\n",
    "    A --> |Hedging, speculation, arbitrage| B\n",
    "    C --> |Features like notional amount, expiration| D\n",
    "    D --> |Market prices, volatility, risk-free rates| E\n",
    "    F --> |Sensitivity to Greeks: Delta, Gamma, Vega, etc.| G\n",
    "    H --> |Adjust based on changing market conditions| I\n",
    "```\n",
    "***Economics:*** \n",
    "```mermaid\n",
    "graph TD;\n",
    "    A[Step 1: Question Breakdown] -->|Extract key terms| A1{Identify Topic}\n",
    "    A1 -->|Micro: Supply & Demand, Market Structures| A2\n",
    "    A1 -->|Macro: GDP, Growth, Policy, Trade| A3\n",
    "    A1 -->|Currency & Regulation| A4\n",
    "\n",
    "    A2 --> B1[Identify model: Elasticity, Cost Curves, Shutdown Points]\n",
    "    A3 --> B2[Map to AD-AS, Business Cycles, Growth Theories]\n",
    "    A4 --> B3[Assess Exchange Rates, Trade, Capital Flows, Regulation]\n",
    "\n",
    "    B1 -->|Check for formula or concept?| C{Numerical or Conceptual}\n",
    "    B2 --> C\n",
    "    B3 --> C\n",
    "\n",
    "    C -->|Numerical| D1[Extract data, apply formulas, check assumptions]\n",
    "    C -->|Conceptual| D2[Analyze cause-effect, policy impact]\n",
    "\n",
    "    D1 --> E[Step 4: Solution Development]\n",
    "    D2 --> E\n",
    "    E -->|Construct structured response| E1(Core insight + economic rationale)\n",
    "    E -->|Consider alternative scenarios| E2(Assess different possibilities)\n",
    "\n",
    "    E1 --> F[Step 5: Answer Validation]\n",
    "    E2 --> F\n",
    "    F -->|Check logic, principles, and assumptions| F1(Verify consistency)\n",
    "    F1 -->|Ensure completeness & clarity| F2(Confirm answer structure)\n",
    "```\n",
    "***Quantitative Methods:*** \n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Articulating Purpose and Context\"] --> B[\"Collecting Input Data\"]\n",
    "    B --> C[\"Processing and Cleaning Data\"]\n",
    "    C --> D[\"Selecting Quantitative Models and Tools\"]\n",
    "    D --> E[\"Estimating Parameters and Testing Hypotheses\"]\n",
    "    E --> F[\"Interpreting Results and Communicating Findings\"]\n",
    "    F --> G[\"Monitoring and Model Reassessment\"]\n",
    "```\n",
    "***Portfolio Management:*** \n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Define Investment Objectives\"] --> B[\"Establish Investment Constraints\"]\n",
    "    B --> C[\"Develop Strategic Asset Allocation\"]\n",
    "    C --> D[\"Incorporate Tactical Adjustments\"]\n",
    "    D --> E[\"Select and Optimize Securities\"]\n",
    "    E --> F[\"Execute Implementation and Trading\"]\n",
    "    F --> G[\"Measure Performance and Attribution\"]\n",
    "    G --> H[\"Monitor Risk and Compliance\"]\n",
    "    H --> I[\"Rebalance and Adjust Portfolio\"]\n",
    "```\n",
    "***Alternative Investments:*** \n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Define Investment Objectives and Mandate\"] --> B[\"Identify Alternative Asset Classes\"]\n",
    "    B --> C[\"Conduct Manager and Strategy Due Diligence\"]\n",
    "    C --> D[\"Perform Valuation and Pricing Analysis\"]\n",
    "    D --> E[\"Assess Risk and Liquidity\"]\n",
    "    E --> F[\"Allocate Alternatives in Portfolio\"]\n",
    "    F --> G[\"Monitor Performance and Rebalance\"]\n",
    "```\n",
    "***Corporate Issuer Analysis:*** \n",
    "```mermaid\n",
    "graph TD\n",
    "    A[\"Corporate Issuer Overview\"] --> B[\"Industry Classification\"]\n",
    "    B --> C[\"Sector Trends and Competitive Landscape\"]\n",
    "    A --> D[\"Financial Statement Analysis\"]\n",
    "    D --> E[\"Profitability, Liquidity, Leverage\"]\n",
    "    A --> F[\"Credit Risk Assessment\"]\n",
    "    F --> G[\"Rating Agencies and Default Probabilities\"]\n",
    "    A --> H[\"Capital Structure and Issuance History\"]\n",
    "    H --> I[\"Bond Issuances and Debt Maturities\"]\n",
    "    A --> J[\"Corporate Governance and Management\"]\n",
    "    J --> K[\"Board Quality and Managerial Competence\"]\n",
    "    A --> L[\"Valuation and Investment Analysis\"]\n",
    "    L --> M[\"DCF, Relative Valuation, Multiples\"]\n",
    "```\n",
    "### Response Format:\n",
    "<thinking>\n",
    "[Think step by step, reflect, and verify the logic behind your answer. Include reasoning here.]\n",
    "</thinking>\n",
    "\n",
    "<output>\n",
    "\"sector\": [The sector being addressed],\n",
    "\"question\": [The financial question],\n",
    "\"answer\": [Only one of the following: \"A\", \"B\", \"C\", \"D\"] \n",
    "</output>\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
